<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Implicit Neural Representations with Periodic Activation Functions">
    <meta name="author" content="Vincent Sitzmann,
                                Julien Martel,
                                Alex Bergman,
                                David Lindell,
								Gordon Wetzstein">

    <title>NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field</h2>
    <h3>Arxiv preprint</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a href="https://sites.google.com/site/lizhong19900216"> Zhong Li</a>,
        <a href="https://lsongx.github.io/"> Liangchen Song</a>,
        <a href="https://www.cct.lsu.edu/~cliu/"> Celong Liu</a>,</br>
        <a href="https://cse.buffalo.edu/~jsyuan/"> Junsong Yuan</a>,
        <a href="https://www.linkedin.com/in/yi-xu-42654823/"> Yi Xu</a>

        <br><a> OPPO US Research Center, Palo Alto, California, USA </a>
        <br><a> University at Buffalo, State University of New York, Buffalo, New York, USA </a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://lizhong3232.github.io/paper/nuelf/NeuLF.pdf">Paper</a>
        <!-- <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a> -->
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/RAu8tjNHyq4" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. 
            In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. 
            For efficient novel view rendering, we adopt a two-plane parameterization of the light field, where each ray is characterized by a 4D parameter. 
            We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function 
            and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably 
            render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with 
            a sparser set of training images. Per-ray depth can be optionally predicted by the network, thus enabling applications such as auto refocus. Our novel view synthesis results are comparable
             to the state-of-the-arts, and even superior in some challenging scenes with refraction and reflection. We achieve this while maintaining an interactive frame rate and a small memory footprint.
        </p>
    </div>

    <div class="section">
        <h2>Demo</h2>
        <hr>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/xiaobu.mp4" type="video/mp4">
                </video>
            </div> 
        </div>

    </div>

    <div class="section">
        <h2>Overview</h2>
        <hr>
        <p>
            For a set of sampled rays from training images, their 4D coordinates and the corresponding color values can be obtained. 
            The input for NeLF is the 4D coordinate of a ray (query) and the output is its RGB color and scene depth. 
            By optimizing the differences between the predicted colors and ground-truth colors, NeLF can faithfully learn the mapping 
            between a 4D coordinate that characterizes the ray and its color. We also build a depth branch to let the network learn 
            the per ray scene depth by self-supervised losses
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/Pipeline.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
    </div>

    <div class="section">
        <h2>Results Video</h2>
        <hr>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/results.mp4" type="video/mp4">
                </video>
            </div> 
        </div>

    </div>

    <div class="section">
        <h2>Qualitative Results</h2>
        <hr>

        <div class="col justify-content-center text-center">
            <img src="img/quantitative.png" style="width:100%; margin-right:-10px; margin-top:10px;">

        <p>
            Qualitative Results on test views from shinny dataset. Our method captures more details on the reflection and refraction areas of the scenes.
        </p>
    </div>


    <!-- <div class="section">
        <h2>Claim</h2>
        <hr>

       

        <p>
            The inital version of this paper was accepted by a top graphics conference, but we decided to retract the submission before camera ready deadline. 
            This is because we found that the simplest nearest neighbor baseline proposed 25 years ago is better than ours on StanfordLF when preparing camera ready version. 
            Publishing this paper will mislead the (neural rendering) field and damage the reputation of the conference, thus we decided to completely retract the paper and 
            refine the work with more solid results. We are extremely sorry that reviewers, area chairs and all other readers wasted time on the initial version.
            Results of the nearest neighbor baseline and the initial version are as follows. (A implementation of the nearest neighbor baseline is included in xxx.)

        </p>
    </div> -->


    <!-- <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="paper/nuelf/NeuLF.pdf"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @misc{chen2021mvsnerf,
                  title={MVSNeRF: Fast Generalizable Radiance Field Reconstruction 
                        from Multi-View Stereo}, 
                  author={Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang 
                          and Fanbo Xiang and Jingyi Yu and Hao Su},
                  year={2021},
                  eprint={2103.15595},
                  archivePrefix={arXiv},
                  primaryClass={cs.CV}
            }
        </div>
    </div> -->

     <footer>
        <p>Send feedback and questions to <a href="https://sites.google.com/site/lizhong19900216/">Zhong Li</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
